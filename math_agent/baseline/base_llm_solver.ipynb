{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2675b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "checkpoint = \"google/gemma-3-1b-it\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b67602b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Evaluate any defined expressions or constants.\n",
    "Substitute known values into subsequent equations where applicable.\n",
    "Solve any resulting systems of equations to find the values of unknowns.\n",
    "If a variable is defined in terms of another (e.g., x=ay+bx=ay+b), solve for the required variable.\n",
    "Present all steps clearly and logically, showing how each result is derived from the previous one.\n",
    "Ensure all algebraic manipulations are valid. Simplify expressions where appropriate.\n",
    "Provide the final answer, along with intermediate steps as needed for clarity. At the end, give just a numerical or symbolic answer.\n",
    "At the end, clearly separate the final numerical answer using the format:\n",
    "FinalAnswer: <value>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf752405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAW OUTPUT:\n",
      "==================================================\n",
      "<bos><bos><start_of_turn>user\n",
      "\n",
      "Evaluate any defined expressions or constants.\n",
      "Substitute known values into subsequent equations where applicable.\n",
      "Solve any resulting systems of equations to find the values of unknowns.\n",
      "If a variable is defined in terms of another (e.g., x=ay+bx=ay+b), solve for the required variable.\n",
      "Present all steps clearly and logically, showing how each result is derived from the previous one.\n",
      "Ensure all algebraic manipulations are valid. Simplify expressions where appropriate.\n",
      "Provide the final answer, along with intermediate steps as needed for clarity. At the end, give just a numerical or symbolic answer.\n",
      "At the end, clearly separate the final numerical answer using the format:\n",
      "FinalAnswer: <value>\n",
      "\n",
      "\n",
      "Solve -20*b + 128*b + 648 = 0 for b.<end_of_turn>\n",
      "**Solution:**\n",
      "\n",
      "We are given the equation -20*b + 128*b + 648 = 0.\n",
      "First, combine the terms with 'b' on the left side:\n",
      "(-20 + 128)b + 648 = 0\n",
      "108b + 648 = 0\n",
      "\n",
      "Now, isolate the term with 'b':\n",
      "108b = -648\n",
      "Divide both sides by 108:\n",
      "b = -648 / 108\n",
      "b = -6\n",
      "\n",
      "Therefore, b = -6.\n",
      "\n",
      "FinalAnswer: -6\n",
      "<end_of_turn>\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "task = \"Solve -20*b + 128*b + 648 = 0 for b.\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": task}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=1024, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "raw_output = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(\"RAW OUTPUT:\")\n",
    "print(\"=\"*50)\n",
    "print(raw_output)\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f17b2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing parser with sample output...\n",
      "Extracted answer: -6\n",
      "Success: True\n",
      "Matches ground truth '-6': True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def parse_llm_output(output_text):\n",
    "    \"\"\"\n",
    "    Parse LLM output to extract the final answer.\n",
    "    \n",
    "    Args:\n",
    "        output_text (str): The raw output from the LLM\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains 'answer', 'reasoning', and 'success' fields\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Remove special tokens and clean the text\n",
    "        cleaned_text = output_text.replace(\"<bos>\", \"\").replace(\"<end_of_turn>\", \"\").strip()\n",
    "        \n",
    "        # Split by user/assistant turns to get only the assistant's response\n",
    "        if \"<start_of_turn>\" in cleaned_text:\n",
    "            parts = cleaned_text.split(\"<start_of_turn>\")\n",
    "            if len(parts) > 1:\n",
    "                assistant_response = parts[-1].strip()\n",
    "            else:\n",
    "                assistant_response = cleaned_text\n",
    "        else:\n",
    "            assistant_response = cleaned_text\n",
    "        \n",
    "        # Extract the final answer using regex\n",
    "        final_answer_patterns = [\n",
    "            r\"FinalAnswer:\\s*([^\\n<]+)\",  # Standard format\n",
    "            r\"Final Answer:\\s*([^\\n<]+)\",  # Alternative format\n",
    "            r\"Answer:\\s*([^\\n<]+)\",  # Simple format\n",
    "            r\"Therefore,?\\s*[a-zA-Z]?\\s*=\\s*([^\\n<]+)\",  # Pattern like \"Therefore, b = -6\"\n",
    "            r\"The answer is\\s*([^\\n<]+)\",  # Natural language format\n",
    "        ]\n",
    "        \n",
    "        extracted_answer = None\n",
    "        for pattern in final_answer_patterns:\n",
    "            match = re.search(pattern, assistant_response, re.IGNORECASE)\n",
    "            if match:\n",
    "                extracted_answer = match.group(1).strip()\n",
    "                break\n",
    "        \n",
    "        # If no pattern matched, try to find the last number or expression\n",
    "        if not extracted_answer:\n",
    "            # Look for patterns like \"b = -6\" or \"x = 5\" at the end\n",
    "            number_pattern = r\"[a-zA-Z]?\\s*=\\s*([-+]?\\d*\\.?\\d+(?:[eE][-+]?\\d+)?)\"\n",
    "            matches = re.findall(number_pattern, assistant_response)\n",
    "            if matches:\n",
    "                extracted_answer = matches[-1]\n",
    "        \n",
    "        # Clean up the extracted answer\n",
    "        if extracted_answer:\n",
    "            extracted_answer = extracted_answer.strip(\".,!?;\")\n",
    "            # Remove any trailing punctuation or markdown\n",
    "            extracted_answer = re.sub(r'[*_`]+', '', extracted_answer)\n",
    "            \n",
    "        return {\n",
    "            'answer': extracted_answer,\n",
    "            'reasoning': assistant_response,\n",
    "            'success': extracted_answer is not None,\n",
    "            'raw_output': output_text\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'answer': None,\n",
    "            'reasoning': output_text,\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'raw_output': output_text\n",
    "        }\n",
    "\n",
    "def evaluate_solution(predicted_answer, ground_truth):\n",
    "    \"\"\"\n",
    "    Evaluate if the predicted answer matches the ground truth.\n",
    "    \n",
    "    Args:\n",
    "        predicted_answer (str): The extracted answer from LLM\n",
    "        ground_truth (str): The correct answer\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if answers match, False otherwise\n",
    "    \"\"\"\n",
    "    if not predicted_answer or not ground_truth:\n",
    "        return False\n",
    "    \n",
    "    # Clean both answers\n",
    "    def clean_answer(ans):\n",
    "        if ans is None:\n",
    "            return None\n",
    "        # Remove whitespace and convert to string\n",
    "        ans = str(ans).strip()\n",
    "        # Remove common formatting\n",
    "        ans = ans.replace(\" \", \"\").replace(\",\", \"\")\n",
    "        return ans.lower()\n",
    "    \n",
    "    pred_clean = clean_answer(predicted_answer)\n",
    "    truth_clean = clean_answer(ground_truth)\n",
    "    \n",
    "    if pred_clean == truth_clean:\n",
    "        return True\n",
    "    \n",
    "    # Try to convert to numbers for comparison\n",
    "    try:\n",
    "        pred_num = float(pred_clean)\n",
    "        truth_num = float(truth_clean)\n",
    "        # Check if numbers are close (handle floating point precision)\n",
    "        return abs(pred_num - truth_num) < 1e-6\n",
    "    except (ValueError, TypeError):\n",
    "        # If conversion fails, do string comparison\n",
    "        return pred_clean == truth_clean\n",
    "\n",
    "print(\"Testing parser with sample output...\")\n",
    "test_output = '''<bos><bos><start_of_turn>user\n",
    "Solve -20*b + 128*b + 648 = 0 for b.<end_of_turn>\n",
    "**Solution:**\n",
    "\n",
    "We are given the equation -20*b + 128*b + 648 = 0.\n",
    "First, combine the terms with 'b' on the left side:\n",
    "(-20 + 128)b + 648 = 0\n",
    "108b + 648 = 0\n",
    "\n",
    "Now, isolate the term with 'b':\n",
    "108b = -648\n",
    "Divide both sides by 108:\n",
    "b = -648 / 108\n",
    "b = -6\n",
    "\n",
    "Therefore, b = -6.\n",
    "\n",
    "FinalAnswer: -6\n",
    "<end_of_turn>'''\n",
    "\n",
    "result = parse_llm_output(test_output)\n",
    "print(f\"Extracted answer: {result['answer']}\")\n",
    "print(f\"Success: {result['success']}\")\n",
    "print(f\"Matches ground truth '-6': {evaluate_solution(result['answer'], '-6')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01a39e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SOLVING: Solve -20*b + 128*b + 648 = 0 for b.\n",
      "==================================================\n",
      "Extracted Answer: -6\n",
      "Expected Answer: -6\n",
      "Correct: True\n",
      "Parsing Success: True\n",
      "\n",
      "==================================================\n",
      "REASONING:\n",
      "==================================================\n",
      "user\n",
      "\n",
      "Evaluate any defined expressions or constants.\n",
      "Substitute known values into subsequent equations where applicable.\n",
      "Solve any resulting systems of equations to find the values of unknowns.\n",
      "If a variable is defined in terms of another (e.g., x=ay+bx=ay+b), solve for the required variable.\n",
      "Present all steps clearly and logically, showing how each result is derived from the previous one.\n",
      "Ensure all algebraic manipulations are valid. Simplify expressions where appropriate.\n",
      "Provide the final ans...\n"
     ]
    }
   ],
   "source": [
    "def solve_math_problem(problem, expected_answer=None):\n",
    "    \"\"\"\n",
    "    Solve a math problem using the LLM and parse the result.\n",
    "    \n",
    "    Args:\n",
    "        problem (str): The math problem to solve\n",
    "        expected_answer (str, optional): The expected answer for evaluation\n",
    "        \n",
    "    Returns:\n",
    "        dict: Contains the parsed result and evaluation metrics\n",
    "    \"\"\"\n",
    "    # Prepare the input\n",
    "    messages = [{\"role\": \"system\", \"content\": prompt}, {\"role\": \"user\", \"content\": problem}]\n",
    "    input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    # Generate response\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=1024, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "    raw_output = tokenizer.decode(outputs[0])\n",
    "    \n",
    "    # Parse the output\n",
    "    parsed_result = parse_llm_output(raw_output)\n",
    "    \n",
    "    # Evaluate if expected answer is provided\n",
    "    if expected_answer is not None:\n",
    "        parsed_result['correct'] = evaluate_solution(parsed_result['answer'], expected_answer)\n",
    "        parsed_result['expected_answer'] = expected_answer\n",
    "    \n",
    "    return parsed_result\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"SOLVING: Solve -20*b + 128*b + 648 = 0 for b.\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "result = solve_math_problem(\"Solve -20*b + 128*b + 648 = 0 for b.\", expected_answer=\"-6\")\n",
    "\n",
    "print(f\"Extracted Answer: {result['answer']}\")\n",
    "print(f\"Expected Answer: {result['expected_answer']}\")\n",
    "print(f\"Correct: {result['correct']}\")\n",
    "print(f\"Parsing Success: {result['success']}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"REASONING:\")\n",
    "print(\"=\"*50)\n",
    "print(result['reasoning'][:500] + \"...\" if len(result['reasoning']) > 500 else result['reasoning'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ddc8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_on_dataset(dataset, max_problems=None, categories=None, verbose=True):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a HuggingFace dataset with category-wise statistics.\n",
    "    \n",
    "    Args:\n",
    "        dataset: HuggingFace dataset with 'question', 'answer', and 'category' fields\n",
    "        max_problems (int, optional): Maximum number of problems to evaluate\n",
    "        categories (list, optional): Specific categories to evaluate (if None, evaluate all)\n",
    "        verbose (bool): Whether to print detailed progress\n",
    "        \n",
    "    Returns:\n",
    "        dict: Evaluation results with overall and category-wise statistics\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    from collections import defaultdict\n",
    "    \n",
    "    # Filter by categories if specified\n",
    "    if categories:\n",
    "        dataset = dataset.filter(lambda x: x['category'] in categories)\n",
    "    \n",
    "    # Limit the number of problems if specified\n",
    "    if max_problems:\n",
    "        dataset = dataset.select(range(min(max_problems, len(dataset))))\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    results = []\n",
    "    category_stats = defaultdict(lambda: {\n",
    "        'total': 0, 'correct': 0, 'parsed': 0, 'errors': 0\n",
    "    })\n",
    "    \n",
    "    total_problems = len(dataset)\n",
    "    print(f\"Evaluating model on {total_problems} problems...\")\n",
    "    \n",
    "    # Process each problem\n",
    "    for i, example in enumerate(dataset):\n",
    "        problem = example['question']\n",
    "        expected_answer = example['answer']\n",
    "        category = example['category']\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nProblem {i+1}/{total_problems} [{category}]: {problem[:50]}...\")\n",
    "        \n",
    "        # Update category total count\n",
    "        category_stats[category]['total'] += 1\n",
    "        \n",
    "        try:\n",
    "            result = solve_math_problem(problem, expected_answer)\n",
    "            \n",
    "            if result['success']:\n",
    "                category_stats[category]['parsed'] += 1\n",
    "                if result['correct']:\n",
    "                    category_stats[category]['correct'] += 1\n",
    "                    if verbose:\n",
    "                        print(f\"✅ CORRECT: {result['answer']}\")\n",
    "                else:\n",
    "                    if verbose:\n",
    "                        print(f\"❌ WRONG: Got {result['answer']}, Expected {expected_answer}\")\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"⚠️ PARSING FAILED\")\n",
    "            \n",
    "            results.append({\n",
    "                'problem': problem,\n",
    "                'expected_answer': expected_answer,\n",
    "                'predicted_answer': result['answer'],\n",
    "                'correct': result.get('correct', False),\n",
    "                'parsed_successfully': result['success'],\n",
    "                'category': category,\n",
    "                'reasoning': result['reasoning']\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            category_stats[category]['errors'] += 1\n",
    "            if verbose:\n",
    "                print(f\"❌ ERROR: {str(e)}\")\n",
    "            results.append({\n",
    "                'problem': problem,\n",
    "                'expected_answer': expected_answer,\n",
    "                'predicted_answer': None,\n",
    "                'correct': False,\n",
    "                'parsed_successfully': False,\n",
    "                'category': category,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    total_correct = sum(stats['correct'] for stats in category_stats.values())\n",
    "    total_parsed = sum(stats['parsed'] for stats in category_stats.values())\n",
    "    total_errors = sum(stats['errors'] for stats in category_stats.values())\n",
    "    \n",
    "    overall_parsing_accuracy = total_parsed / total_problems if total_problems > 0 else 0\n",
    "    overall_solving_accuracy = total_correct / total_problems if total_problems > 0 else 0\n",
    "    overall_conditional_accuracy = total_correct / total_parsed if total_parsed > 0 else 0\n",
    "    \n",
    "    # Calculate category-wise metrics\n",
    "    category_metrics = {}\n",
    "    for category, stats in category_stats.items():\n",
    "        parsing_acc = stats['parsed'] / stats['total'] if stats['total'] > 0 else 0\n",
    "        solving_acc = stats['correct'] / stats['total'] if stats['total'] > 0 else 0\n",
    "        conditional_acc = stats['correct'] / stats['parsed'] if stats['parsed'] > 0 else 0\n",
    "        \n",
    "        category_metrics[category] = {\n",
    "            'total_problems': stats['total'],\n",
    "            'correct_answers': stats['correct'],\n",
    "            'parsed_successfully': stats['parsed'],\n",
    "            'errors': stats['errors'],\n",
    "            'parsing_accuracy': parsing_acc,\n",
    "            'solving_accuracy': solving_acc,\n",
    "            'conditional_accuracy': conditional_acc\n",
    "        }\n",
    "    \n",
    "    # Create summary\n",
    "    summary = {\n",
    "        'overall_metrics': {\n",
    "            'total_problems': total_problems,\n",
    "            'parsed_successfully': total_parsed,\n",
    "            'correct_answers': total_correct,\n",
    "            'errors': total_errors,\n",
    "            'parsing_accuracy': overall_parsing_accuracy,\n",
    "            'solving_accuracy': overall_solving_accuracy,\n",
    "            'conditional_accuracy': overall_conditional_accuracy\n",
    "        },\n",
    "        'category_metrics': category_metrics,\n",
    "        'detailed_results': results\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    if verbose:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"EVALUATION SUMMARY\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Overall Results:\")\n",
    "        print(f\"  Total Problems: {total_problems}\")\n",
    "        print(f\"  Successfully Parsed: {total_parsed} ({overall_parsing_accuracy:.2%})\")\n",
    "        print(f\"  Correct Answers: {total_correct} ({overall_solving_accuracy:.2%})\")\n",
    "        print(f\"  Accuracy (given successful parsing): {overall_conditional_accuracy:.2%}\")\n",
    "        print(f\"  Errors: {total_errors}\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"CATEGORY-WISE RESULTS\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Create a nice table for category results\n",
    "        category_data = []\n",
    "        for category, metrics in category_metrics.items():\n",
    "            category_data.append([\n",
    "                category,\n",
    "                metrics['total_problems'],\n",
    "                metrics['correct_answers'],\n",
    "                f\"{metrics['solving_accuracy']:.2%}\",\n",
    "                f\"{metrics['parsing_accuracy']:.2%}\",\n",
    "                f\"{metrics['conditional_accuracy']:.2%}\"\n",
    "            ])\n",
    "        \n",
    "        # Sort by category name\n",
    "        category_data.sort(key=lambda x: x[0])\n",
    "        \n",
    "        # Print table header\n",
    "        print(f\"{'Category':<15} {'Total':<7} {'Correct':<7} {'Solve%':<7} {'Parse%':<7} {'Cond%':<7}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Print category results\n",
    "        for row in category_data:\n",
    "            print(f\"{row[0]:<15} {row[1]:<7} {row[2]:<7} {row[3]:<7} {row[4]:<7} {row[5]:<7}\")\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def save_evaluation_results(summary, filename=\"evaluation_results.json\"):\n",
    "    \"\"\"Save evaluation results to a JSON file.\"\"\"\n",
    "    import json\n",
    "    \n",
    "    # Create a serializable version (remove non-serializable parts)\n",
    "    serializable_summary = {\n",
    "        'overall_metrics': summary['overall_metrics'],\n",
    "        'category_metrics': summary['category_metrics'],\n",
    "        'detailed_results': [{\n",
    "            'problem': r['problem'],\n",
    "            'expected_answer': r['expected_answer'],\n",
    "            'predicted_answer': r['predicted_answer'],\n",
    "            'correct': r['correct'],\n",
    "            'parsed_successfully': r['parsed_successfully'],\n",
    "            'category': r['category']\n",
    "        } for r in summary['detailed_results']]\n",
    "    }\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(serializable_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Results saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97d7091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Total examples: 1214\n",
      "Example: {'question': 'What is the greatest common factor of 13975 and 130?', 'answer': '65', 'category': 'numbers'}\n",
      "Categories: {'arithmetic', 'algebra', 'probability', 'comparison', 'measurement', 'numbers', 'polynomials', 'calculus'}\n",
      "\n",
      "================================================================================\n",
      "TESTING WITH A SMALL SAMPLE (5 problems)\n",
      "================================================================================\n",
      "Evaluating model on 5 problems...\n",
      "\n",
      "Problem 1/5 [numbers]: What is the greatest common factor of 13975 and 13...\n",
      "❌ WRONG: Got 5, Expected 65\n",
      "\n",
      "Problem 2/5 [numbers]: What is the highest common factor of 20 and 365?...\n",
      "✅ CORRECT: 5\n",
      "\n",
      "Problem 3/5 [numbers]: Calculate the greatest common factor of 34945 and ...\n",
      "❌ WRONG: Got 3, Expected 145\n",
      "\n",
      "Problem 4/5 [numbers]: Let m(t) = -6*t - 3. Let o be m(-6). Let r = o - 7...\n",
      "❌ WRONG: Got -18, Expected 2\n",
      "\n",
      "Problem 5/5 [numbers]: What is the common denominator of 127/12 and 55/36...\n",
      "❌ WRONG: Got 3001, Expected 12\n",
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Overall Results:\n",
      "  Total Problems: 5\n",
      "  Successfully Parsed: 5 (100.00%)\n",
      "  Correct Answers: 1 (20.00%)\n",
      "  Accuracy (given successful parsing): 20.00%\n",
      "  Errors: 0\n",
      "\n",
      "================================================================================\n",
      "CATEGORY-WISE RESULTS\n",
      "================================================================================\n",
      "Category        Total   Correct Solve%  Parse%  Cond%  \n",
      "--------------------------------------------------------------------------------\n",
      "numbers         5       1       20.00%  100.00% 20.00% \n",
      "\n",
      "================================================================================\n",
      "QUICK CATEGORY TEST (2 problems from 'arithmetic')\n",
      "================================================================================\n",
      "Evaluating model on 2 problems...\n",
      "\n",
      "Problem 1/2 [arithmetic]: Simplify (-4 + sqrt(208)*-1)**2 + (-1 + sqrt(26)/s...\n",
      "❌ WRONG: Got 248.21, Expected 30*sqrt(13) + 235\n",
      "\n",
      "Problem 2/2 [arithmetic]: Simplify sqrt(147) + (sqrt(147) + (sqrt(147) - (1 ...\n",
      "❌ WRONG: Got 2, Expected 124*sqrt(3) + 592\n",
      "\n",
      "================================================================================\n",
      "EVALUATION SUMMARY\n",
      "================================================================================\n",
      "Overall Results:\n",
      "  Total Problems: 2\n",
      "  Successfully Parsed: 2 (100.00%)\n",
      "  Correct Answers: 0 (0.00%)\n",
      "  Accuracy (given successful parsing): 0.00%\n",
      "  Errors: 0\n",
      "\n",
      "================================================================================\n",
      "CATEGORY-WISE RESULTS\n",
      "================================================================================\n",
      "Category        Total   Correct Solve%  Parse%  Cond%  \n",
      "--------------------------------------------------------------------------------\n",
      "arithmetic      2       0       0.00%   100.00% 0.00%  \n",
      "Results saved to sample_evaluation_results.json\n",
      "\n",
      "Sample evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "# Example usage with the HuggingFace dataset\n",
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the dataset\n",
    "dataset = load_from_disk(\"../../data/processed/math_qa_dataset\")\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Total examples: {len(dataset)}\")\n",
    "print(f\"Example: {dataset[0]}\")\n",
    "print(f\"Categories: {set(dataset['category'])}\")\n",
    "\n",
    "# Test with a small sample first\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TESTING WITH A SMALL SAMPLE (5 problems)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample_results = evaluate_model_on_dataset(dataset, max_problems=5, verbose=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"QUICK CATEGORY TEST (2 problems from 'arithmetic')\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test with specific categories\n",
    "category_results = evaluate_model_on_dataset(\n",
    "    dataset, \n",
    "    max_problems=2, \n",
    "    categories=['arithmetic'], \n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Save results\n",
    "save_evaluation_results(sample_results, \"sample_evaluation_results.json\")\n",
    "print(\"\\nSample evaluation complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
